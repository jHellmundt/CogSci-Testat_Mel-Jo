{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IANNWTF - HW10_word2vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO9ixc6Eili5Z6ynvnIUKKS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jHellmundt/CogSci-Testat_Mel-Jo/blob/master/IANNWTF_HW10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ypXFj0LKwS"
      },
      "source": [
        "import numpy as np\r\n",
        "#%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import math\r\n",
        "from tensorflow.keras import layers\r\n",
        "from collections import Counter\r\n",
        "from time import perf_counter"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4wzDGdTme2"
      },
      "source": [
        "# The number of the most common words to keep\r\n",
        "NUM_WORDS = 10000 #@param\r\n",
        "\r\n",
        "# Only even word windows allowed (will be downscaled to the next even number)\r\n",
        "WORD_WIN = 4 #@param\r\n",
        "BATCH_SIZE = 128 #@param\r\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2xUDe9DLPs4"
      },
      "source": [
        "# Since tfds datasets don't make any sense we get the data in the form a normal person would get it -> txt\r\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\r\n",
        "data_og = open(path_to_file, 'rb').read().decode(encoding='utf-8')\r\n",
        "\r\n",
        "# Make the whole data lowercase\r\n",
        "data = data_og.lower()\r\n",
        "\r\n",
        "# Remove new-line marks and all punctuation\r\n",
        "data = re.sub(\"\\n+\", \" \", data)\r\n",
        "data = re.sub(r\"[.|,|;|:|!|?|\\-\\-]\", \"\", data)\r\n",
        "\r\n",
        "# Tokenize\r\n",
        "data = re.split(r\"\\ +\", data[:-1])\r\n",
        "\r\n",
        "# Take only the n most common words\r\n",
        "word_counts = Counter(data)\r\n",
        "total_word_count = len(data)\r\n",
        "token = np.array(word_counts.most_common(NUM_WORDS))[:,0]\r\n",
        "\r\n",
        "# Create unigrams which contain the probability of each token based on word frequency\r\n",
        "# This is later used for Negative Sampling\r\n",
        "unigram = np.array(word_counts.most_common(NUM_WORDS))[:,1].astype(\"int32\")\r\n",
        "unigram = list(unigram / np.sum(unigram))\r\n",
        "\r\n",
        "# Create token to ID and ID to token Dictionaries\r\n",
        "token2id = dict(zip(np.concatenate([[\"<UNK>\"], token]), range(len(token))))\r\n",
        "id2token = dict([(token2id[token], token) for token in token2id.keys()])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSUbh6EhWMiP"
      },
      "source": [
        "def subsampler(word, s=0.001):\r\n",
        "  freq = word_counts[word] / total_word_count\r\n",
        "  prob = (math.sqrt(freq/s) + 1) * (s/freq)\r\n",
        "  return np.random.random() <= prob\r\n",
        "\r\n",
        "# Define the relative word window ids\r\n",
        "word_win_ids = np.array([[x,-x] for x in range(1,int(WORD_WIN/2)+1)]).reshape(-1)\r\n",
        "\r\n",
        "# Create the training data using the word window\r\n",
        "data_train = []\r\n",
        "for i in range(len(data)):\r\n",
        "  for j in word_win_ids:\r\n",
        "    try:\r\n",
        "      if(subsampler(data[i+j]) and i+j >= 0 and i+j <= len(data)):\r\n",
        "        data_train.append((token2id[data[i]], token2id[data[i+j]]))\r\n",
        "    except:\r\n",
        "      pass\r\n",
        "\r\n",
        "data_train = np.array(data_train)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIFD2sHMj1qD"
      },
      "source": [
        "# Create a Tensorflow Dataset for Training the SKIP-GRAM\r\n",
        "data_train = tf.data.Dataset.from_tensor_slices((data_train[:,0], data_train[:,1]))\r\n",
        "data_train = data_train.shuffle(1000).batch(BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvL862rjm9Iv"
      },
      "source": [
        "class SkipGram(layers.Layer):\r\n",
        "  def __init__(self, e_size, v_size):\r\n",
        "    super(SkipGram, self).__init__()\r\n",
        "\r\n",
        "    self.e_size = e_size\r\n",
        "    self.v_size = v_size\r\n",
        "\r\n",
        "  def build(self, _):\r\n",
        "    self.embedding_matrix = self.add_weight(\r\n",
        "                              shape=(self.v_size, self.e_size),\r\n",
        "                              initializer='RandomNormal'\r\n",
        "                            )\r\n",
        "    self.score_matrix = self.add_weight(\r\n",
        "                          shape=(self.v_size, self.e_size),\r\n",
        "                          initializer='RandomNormal'\r\n",
        "                        )\r\n",
        "    self.score_bias = self.add_weight(\r\n",
        "                        shape=(self.v_size),\r\n",
        "                        initializer='zeros'\r\n",
        "                      )\r\n",
        "\r\n",
        "  #@tf.function\r\n",
        "  def call(self, target, context):\r\n",
        "    batch_size = tf.shape(context)[0]\r\n",
        "    context = tf.reshape(context, (batch_size, 1))\r\n",
        "\r\n",
        "    target_embedding = tf.nn.embedding_lookup(self.embedding_matrix, target)\r\n",
        "    target_embedding = tf.reshape(target_embedding, shape=(batch_size, self.e_size))\r\n",
        "\r\n",
        "    loss = tf.reduce_mean(\r\n",
        "        tf.nn.nce_loss(weights=self.score_matrix,\r\n",
        "                       biases=self.score_bias,\r\n",
        "                       labels=context,\r\n",
        "                       inputs=target_embedding,\r\n",
        "                       num_sampled = 1,\r\n",
        "                       num_classes = self.v_size,\r\n",
        "                       num_true=1,\r\n",
        "                       sampled_values=tf.random.fixed_unigram_candidate_sampler(\r\n",
        "                           true_classes=context, # Maybe use matrix of all words that ever appeared next to the target word\r\n",
        "                           num_true=1,\r\n",
        "                           num_sampled=1,\r\n",
        "                           unique=False,\r\n",
        "                           range_max=self.v_size,\r\n",
        "                           unigrams=unigram\r\n",
        "                       ))\r\n",
        "    )\r\n",
        "\r\n",
        "    return loss"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMAJsWu-ubUj"
      },
      "source": [
        "def train_step(model, target, context, optimizer):\r\n",
        "  # Train the model using gradient tape and return the loss for visualisation\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    loss = model(target, context)\r\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\r\n",
        "\r\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\r\n",
        "\r\n",
        "  return loss \r\n",
        "\r\n",
        "def nearest_neighbours(target, k, embedding_matrix):\r\n",
        "  cos_sims = []\r\n",
        "  target_embedding = tf.nn.embedding_lookup(embedding_matrix, token2id[target])\r\n",
        "  for e in embedding_matrix.numpy():\r\n",
        "    cos_sims.append(np.dot(target_embedding, e) / (np.linalg.norm(target_embedding) * np.linalg.norm(e)))\r\n",
        "  k_best_neighbours = np.argsort(-cos_sims)[1:k+1]\r\n",
        "\r\n",
        "  return [id2token[n] for n in k_best_neighbours]\r\n",
        "\r\n",
        "def train_model(num_epochs, learning_rate, model, test_tokens, num_neighbours):\r\n",
        "  tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "  running_average_factor = 0.95\r\n",
        "\r\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
        "\r\n",
        "  train_losses = []\r\n",
        "  \r\n",
        "  # TODO: What are these warnings??\r\n",
        "  tf.get_logger().setLevel(\"ERROR\")\r\n",
        "  e = []\r\n",
        "  # Train the model (record the time as well for performance judgements)\r\n",
        "  for epoch in range(1, num_epochs + 1):\r\n",
        "      start = perf_counter()\r\n",
        "\r\n",
        "      average = []\r\n",
        "      for (target, context) in data_train:\r\n",
        "          train_loss = train_step(model, target, context, optimizer)\r\n",
        "          average.append(train_loss)\r\n",
        "          \r\n",
        "      train_losses.append(np.mean(average))\r\n",
        "\r\n",
        "      print(f\"Epoch #{epoch}:\" + \" \" * (len(str(num_epochs)) - len(str(epoch))) + f\"Loss: {'{0:.3f}'.format(round(float(train_losses[-1]), 3))}  Time: {'{0:.2f}'.format(round(perf_counter() - start, 2))}s\")\r\n",
        "      for token in test_tokens:\r\n",
        "        print(f\"  {token}: {', '.join(nearest_neighbours(token, num_neighbours, model.embedding_matrix))}\")\r\n",
        "      e.append(model.embedding_matrix)\r\n",
        "      \r\n",
        "  return train_losses, e\r\n",
        "\r\n",
        "def plot_learning(train_losses, num_epochs):\r\n",
        "  # draw the loss plot\r\n",
        "  line1, = plt.plot(train_losses)\r\n",
        "  plt.xlabel(\"Epochs\")\r\n",
        "  plt.ylabel(\"Loss\")\r\n",
        "  plt.show()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaNwip-re-P6"
      },
      "source": [
        "num_epochs =  3#@param\r\n",
        "learning_rate = 1 #@param\r\n",
        "embedding_size = 64 #@param\r\n",
        "num_best_neighbours = 5 #@param\r\n",
        "\r\n",
        "test_tokens = [\"queen\", \"throne\", \"wine\", \"poison\", \"love\", \"strong\", \"day\"]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT8K0yl8SsJv"
      },
      "source": [
        "tf.keras.backend.clear_session()\r\n",
        "\r\n",
        "model = SkipGram(embedding_size, NUM_WORDS)\r\n",
        "\r\n",
        "train_losses, e = train_model(num_epochs, learning_rate, model, test_tokens, num_best_neighbours)\r\n",
        "\r\n",
        "plot_learning(train_losses, num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek0xfacRc9Hx"
      },
      "source": [
        "# To show you whats going wrong here are the embedding matrices of each epoch\r\n",
        "for i in e:\r\n",
        "  print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXVturK6q4TB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}